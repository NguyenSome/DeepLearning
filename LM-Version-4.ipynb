{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7bcc19b8",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# for recrod keeping purposes\n",
    "import time\n",
    "\n",
    "from comet_ml import Experiment\n",
    "\n",
    "import torch\n",
    "from sklearn.model_selection import train_test_split\n",
    "import torchvision\n",
    "import torchvision.transforms as transforms\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "%matplotlib inline\n",
    "\n",
    "## Dependencies\n",
    "print(\"Using torch\", torch.__version__)\n",
    "print(\"Cuda version is:\", torch.version.cuda)\n",
    "# print(\"cuDNN version is :\", torch.backends.cudnn.version())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "75bbd3b9",
   "metadata": {},
   "source": [
    "I used Comet ML for ease of graphing. I've also added some matplot graphs below to check for differences between the two and found none. Work produced in CometML linked below for all testing graphs.\n",
    "\n",
    "https://www.comet.ml/nguyensome/dl/view/new/panels\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a4a1b422",
   "metadata": {},
   "outputs": [],
   "source": [
    "experiment = Experiment(\n",
    "    api_key=\"dBen8W4vvf6ErSgyIaZZhL9UG\",\n",
    "    project_name=\"dl\",\n",
    "    workspace=\"nguyensome\",\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eef1c114",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Setting hyperparameters & device\n",
    "device = torch.device(\"cuda\") if torch.cuda.is_available() else torch.device(\"cpu\")\n",
    "print(\"Device\", device)\n",
    "torch.manual_seed(3)\n",
    "                  \n",
    "## Global hyperparamters set here after hyperparameter tuning\n",
    "batch_size = 128\n",
    "learning_rate = 1e-4\n",
    "num_epochs = 100\n",
    "num_classes =  100"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6239e1e9",
   "metadata": {},
   "source": [
    "1) Load CIFAR100 and split into train, validation, test sets. Dataloader will handle shuffling, batching, etc.\n",
    "\n",
    "2) Random Erasing selected as suggested by: https://journalofbigdata.springeropen.com/articles/10.1186/s40537-019-0197-0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d4c6d8d0",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "transform =  transforms.Compose([\n",
    "    transforms.ToTensor(),\n",
    "    transforms.RandomHorizontalFlip(0.4),\n",
    "    transforms.RandomErasing(),\n",
    "    transforms.Normalize((0.5, 0.5, 0.4), (0.225, 0.225, 0.225))\n",
    "])\n",
    "\n",
    "## Load test data\n",
    "testset = torchvision.datasets.CIFAR100(root='./data', train= False,\n",
    "                                       download=True, transform=transform)\n",
    "testloader = torch.utils.data.DataLoader(testset, batch_size=batch_size,\n",
    "                                        shuffle=False, num_workers=2, pin_memory=True)\n",
    "\n",
    "## Split training data into training and validation sets 4:1\n",
    "trainset =  torchvision.datasets.CIFAR100(root='./data', train = True,\n",
    "                                         download=True, transform=transform)\n",
    "\n",
    "train_indices, val_indices = train_test_split(list(range(len(trainset.targets))), \n",
    "                                              test_size=0.2, stratify=trainset.targets)\n",
    "train = torch.utils.data.Subset(trainset, train_indices)\n",
    "val = torch.utils.data.Subset(trainset, val_indices)\n",
    "\n",
    "trainloader = torch.utils.data.DataLoader(train, batch_size=batch_size,\n",
    "                                         shuffle=True, num_workers=2, pin_memory=True)\n",
    "valloader = torch.utils.data.DataLoader(val, batch_size=batch_size,\n",
    "                                         shuffle=True, num_workers=2, pin_memory=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d53f01c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(len(trainloader))\n",
    "print(len(valloader))\n",
    "print(len(testloader))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "99e2f016",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "## Function to train barebone model for hyperparameter testing\n",
    "def trainhyp(model, learning_rate, batch_size, optimizer, num_epoch):\n",
    "    \n",
    "    transform =  transforms.Compose([\n",
    "    transforms.ToTensor(),\n",
    "    ])\n",
    "\n",
    "    testset = torchvision.datasets.CIFAR100(root='./data', train= False,\n",
    "                                           download=True, transform=transform)\n",
    "    testloader = torch.utils.data.DataLoader(testset, batch_size=batch_size,\n",
    "                                            shuffle=False, num_workers=2, pin_memory=True)\n",
    "\n",
    "    trainset =  torchvision.datasets.CIFAR100(root='./data', train = True,\n",
    "                                             download=True, transform=transform)\n",
    "\n",
    "    train_indices, val_indices = train_test_split(list(range(len(trainset.targets))), \n",
    "                                                  test_size=0.2, stratify=trainset.targets)\n",
    "    train = torch.utils.data.Subset(trainset, train_indices)\n",
    "    val = torch.utils.data.Subset(trainset, val_indices)\n",
    "\n",
    "    trainloader = torch.utils.data.DataLoader(train, batch_size=batch_size,\n",
    "                                             shuffle=True, num_workers=2, pin_memory=True)\n",
    "    valloader = torch.utils.data.DataLoader(val, batch_size=batch_size,\n",
    "                                             shuffle=True, num_workers=2, pin_memory=True)\n",
    "\n",
    "    \n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "    \n",
    "    if optimizer == \"Adam\":\n",
    "        optimizer = optim.Adam(model.parameters(), lr=learning_rate, weight_decay = 1e-2)\n",
    "    elif optimizer == \"SGD\":\n",
    "        optimizer = optim.SGD(model.parameters(), lr=learning_rate, weight_decay = 1e-2)\n",
    "    else:\n",
    "        optimizer = optim.Adagrad(model.parameters(), lr=learning_rate, weight_decay = 1e-2)\n",
    "        \n",
    "    for epoch in range(num_epoch):\n",
    "\n",
    "        model = model.train()\n",
    "        \n",
    "        for t, (images, labels) in enumerate(trainloader):\n",
    "            images = images.to(device)\n",
    "            labels = labels.to(device)\n",
    "            optimizer.zero_grad()\n",
    "\n",
    "            # forward, loss, backprop\n",
    "            logits = model(images)\n",
    "            loss = criterion(logits, labels)\n",
    "            \n",
    "            loss.backward()\n",
    "\n",
    "            optimizer.step()\n",
    "\n",
    "    return testloader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "158dbdce",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_lrs(model):\n",
    "\n",
    "    avg_train_loss = []\n",
    "    avg_val_loss = []\n",
    "    val_loss = []\n",
    "    train_loss = []\n",
    "    val_acc = []\n",
    "    train_acc = []\n",
    "    \n",
    "    criterion = nn.CrossEntropyLoss().to(device)\n",
    "    optimizer = optim.Adam(model.parameters(), lr=learning_rate, weight_decay = 1e-2)\n",
    "    schedular = optim.lr_scheduler.ReduceLROnPlateau(optimizer)\n",
    "    \n",
    "\n",
    "    for epoch in range(num_epochs):\n",
    "        # Clear list each episode\n",
    "        train_loss = []\n",
    "        val_loss = []\n",
    "\n",
    "        model = model.train()\n",
    "        \n",
    "        for t, (images, labels) in enumerate(trainloader):\n",
    "            images = images.to(device)\n",
    "            labels = labels.to(device)\n",
    "            optimizer.zero_grad()\n",
    "\n",
    "            # forward, loss, backprop\n",
    "            logits = model(images)\n",
    "            loss = criterion(logits, labels)\n",
    "            experiment.log_metric(\"Training Loss\", loss.item())\n",
    "            \n",
    "            train_loss.append(loss.item())\n",
    "            loss.backward()\n",
    "\n",
    "            optimizer.step()\n",
    "        \n",
    "        # Evaluating w/ validation set\n",
    "        model.eval()\n",
    "\n",
    "        with torch.no_grad():\n",
    "            for v, (images, labels) in enumerate(valloader):\n",
    "                images = images.to(device)\n",
    "                labels = labels.to(device) \n",
    "                logits = model(images)\n",
    "\n",
    "                loss = criterion(logits, labels)\n",
    "                experiment.log_metric(\"Validation Loss\", loss.item())\n",
    "                val_loss.append(loss.item())\n",
    "        \n",
    "        schedular.step(loss)\n",
    "        accuracy = evaluate(valloader, model)\n",
    "        experiment.log_metric(\"Validation Accuracy\", accuracy, epoch=epoch)\n",
    "        val_acc.append(accuracy)\n",
    "        \n",
    "        acc = evaluate(trainloader, model)\n",
    "        experiment.log_metric(\"Training Accuracy\", acc, epoch=epoch)\n",
    "        train_acc.append(acc)\n",
    "        \n",
    "        train_l = np.average(train_loss)\n",
    "        val_l = np.average(val_loss)\n",
    "        avg_train_loss.append(train_l)\n",
    "        avg_val_loss.append(val_l)\n",
    "\n",
    "        print('Epoch : %d/%d | Train Loss : %.2f | Val Loss : %.2f | Val Acc : %.3f' \n",
    "              % (epoch, num_epochs, train_l, val_l, accuracy))\n",
    "\n",
    "    return avg_train_loss, avg_val_loss, val_acc, train_acc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b1d57d29",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Accuracy testing regular for training purposes as well as top 1 and 5 for testing purposes\n",
    "def evaluate(dataset, model):\n",
    "    model = model\n",
    "\n",
    "    test_acc, total, correct = 0, 0, 0\n",
    "    \n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        for images, labels in dataset:\n",
    "            images = images.to(device)\n",
    "            labels = labels.to(device)\n",
    "            logits = model(images)\n",
    "\n",
    "            _, predicted =torch.max(logits, 1)\n",
    "            correct += (predicted == labels.data).sum()\n",
    "            \n",
    "    total = len(dataset.dataset)\n",
    "    test_acc = (correct/total).item()\n",
    "    \n",
    "    return test_acc      \n",
    "\n",
    "def top_evaluate(dataset, model):\n",
    "    top1, top5, count = 0, 0 , 0\n",
    "    with torch.no_grad():\n",
    "        for images, labels in dataset:\n",
    "            images = images.to(device)\n",
    "            labels = labels.to(device)\n",
    "            logits = model(images)\n",
    "\n",
    "            _, predicted =torch.max(logits, 1)\n",
    "            top1 += (predicted == labels.data).sum()\n",
    "            count += predicted.shape[0]\n",
    "    acc_top = top1/count\n",
    "    print(\"Top-1 error: %.2f\" % (acc_top ))\n",
    "    \n",
    "    return acc_top"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c8fc622a",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Function to find learning rate\n",
    "def LRFind(model, learning_rate):\n",
    "    \n",
    "    avg_train_loss = []\n",
    "    train_loss = []\n",
    "    \n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "    optimizer = optim.Adam(model.parameters(), lr=learning_rate)\n",
    "\n",
    "    for epoch in range(num_epochs):\n",
    "        # Clear list each episode\n",
    "        train_loss = []\n",
    "        model = model.train()\n",
    "        \n",
    "        for t, (images, labels) in enumerate(trainloader):\n",
    "            images = images.to(device)\n",
    "            labels = labels.to(device)\n",
    "            optimizer.zero_grad()\n",
    "\n",
    "            # forward, loss, backprop\n",
    "            logits = model(images)\n",
    "            loss = criterion(logits, labels)\n",
    "            experiment.log_metric(name=learning_rate, value=loss.item() )\n",
    "            \n",
    "            train_loss.append(loss.item())\n",
    "            loss.backward()\n",
    "\n",
    "            optimizer.step()\n",
    "        \n",
    "        train_l = np.average(train_loss)\n",
    "        avg_train_loss.append(train_l)\n",
    "\n",
    "        print('Epoch : %d/%d | Train Loss : %.2f' \n",
    "              % (epoch, num_epochs, train_l))\n",
    "\n",
    "    return avg_train_loss"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4e46e909",
   "metadata": {},
   "source": [
    "Grid search will find initial hyperparamters for the model. Parameters such as weight decay, momentum, etc are to be manually tested after."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "675d0402",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Grid search function over select hyperparameters\n",
    "def GridSearch(net_type):\n",
    "    lrs = [1e-3, 1e-4, 5e-4, 1e-5]\n",
    "    optimizers = [\"Adam\", \"Adagrad\", \"SDG\"]\n",
    "    batch_sizes = [32, 64, 128]\n",
    "\n",
    "    Dict = {}\n",
    "    s=''\n",
    "    epoch = 10\n",
    "    \n",
    "    for lr in lrs:\n",
    "            for batch_size in batch_sizes:\n",
    "                for opt in optimizers:\n",
    "                    \n",
    "                    # determine model\n",
    "                    if net_type == 'LN':\n",
    "                        model = LinearNet()\n",
    "                        print('LN')\n",
    "                    elif net_type == 'CN':\n",
    "                        model = ConvNet()\n",
    "                        print('CN')\n",
    "                    else:\n",
    "                        print('GN')\n",
    "                        model = models.googlenet(aux_logits = False)\n",
    "                        for param in model.parameters():\n",
    "                            param.grad_requires = False\n",
    "\n",
    "                        in_features = model.fc.in_features\n",
    "                        model.fc = nn.Linear(in_features, 100, bias=True)\n",
    "\n",
    "                    model = model.to(device)\n",
    "                    \n",
    "                    # Record keeping dictionary and Comet\n",
    "                    s =''.join([str(lr), str(opt), str(batch_size)])\n",
    "                    experiment.set_name(s)\n",
    "                    \n",
    "                    testset = trainhyp(model, lr, batch_size, opt, epoch)\n",
    "                    test_acc = evaluate(testset, model)\n",
    "                    Dict[s] = test_acc\n",
    "                    \n",
    "    return max(Dict, key=Dict.get), Dict"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "35237b40",
   "metadata": {},
   "source": [
    "Part 1: Linear Model. Model has input layer, 1 FC layer, output layer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b0099c7b",
   "metadata": {},
   "outputs": [],
   "source": [
    "class LinearNet(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(LinearNet, self).__init__()\n",
    "        self.layers = nn.Sequential(\n",
    "#             nn.Linear(32*32*3, 64*64*10), #input layer\n",
    "#             nn.ReLU(),\n",
    "#             nn.Linear(64*64*10, num_classes)\n",
    "#         )\n",
    "        ## temp test for lr\n",
    "            nn.Linear(32*32*3, 32*32*10), #input layer\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(32*32*10, num_classes)\n",
    "        )\n",
    "        \n",
    "    # Output tensor\n",
    "    def forward(self, x):\n",
    "        x = x.view(x.size(0), -1)\n",
    "        return self.layers(x)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5f90f69f",
   "metadata": {},
   "source": [
    "Below are the functions to run either a hyperparamter grid search for linear net or a learning rate search. No model initialization needed beforehand."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4d7ce731",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# ## LR selection for LN ##\n",
    "# def LNRateSelection(param):\n",
    "#     torch.cuda.empty_cache()\n",
    "#     lr_list = []\n",
    "#     for p in param:\n",
    "#         #for each learning rate, instantiate a new model\n",
    "#         modelLN = LinearNet()\n",
    "#         modelLN = modelLN.to(device)\n",
    "#         train_loss = LRFind(modelLN, p)\n",
    "#         lr_list.append(train_loss)\n",
    "    \n",
    "#     return lr_list\n",
    "\n",
    "# # Testing LR hyperparameters\n",
    "# parameters = [1e-3, 1e-4, 1e-5, 1e-6, 1e-7]\n",
    "\n",
    "# LNArray = LNRateSelection(parameters)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5eceb473",
   "metadata": {},
   "source": [
    "Optimal learning rate selected to be 1e-4 for Linear Network. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5b84ae32",
   "metadata": {},
   "outputs": [],
   "source": [
    "# %%time\n",
    "# value, Dict = GridSearch('LN')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3f033626",
   "metadata": {},
   "outputs": [],
   "source": [
    "# print(value)\n",
    "# print(Dict[value])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "35f2ee38",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "%%time\n",
    "## Approximately 45mins for 100 epochs\n",
    "\n",
    "modelLN = LinearNet()\n",
    "modelLN = modelLN.to(device)\n",
    "print(modelLN)\n",
    "a, b, c, d = train_lrs(modelLN)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "94f7e7c9",
   "metadata": {},
   "source": [
    "Sample of an accuracy plot using matplot below for demonstration purposes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0a4bd6fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# plt.plot(c, label = 'Training Accuracy')\n",
    "# plt.plot(d, label = 'Validation Accuracy')\n",
    "# plt.legend()\n",
    "# plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f537fbc1",
   "metadata": {},
   "source": [
    "Test accuracy with the final model was about 22% and 26% test and training respectively on 100 epochs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e787adc2",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Evaluating model with test set ##\n",
    "test_acc = evaluate(testloader, modelLN)\n",
    "train_acc = evaluate(trainloader, modelLN)\n",
    "print(test_acc, train_acc)\n",
    "\n",
    "_ = top_evaluate(testloader, modelLN)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8b504a7a",
   "metadata": {},
   "source": [
    "Run these two blocks to start a new comment experiment for next model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e715df29",
   "metadata": {},
   "outputs": [],
   "source": [
    "# del modelLN\n",
    "# experiment.end()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a95e6577",
   "metadata": {},
   "outputs": [],
   "source": [
    "# experiment = Experiment(\n",
    "#     api_key=\"dBen8W4vvf6ErSgyIaZZhL9UG\",\n",
    "#     project_name=\"dl\",\n",
    "#     workspace=\"nguyensome\",\n",
    "# )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4fc7efe9",
   "metadata": {},
   "source": [
    "Part 2: CNN with 6 layers. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4de41fe8",
   "metadata": {},
   "outputs": [],
   "source": [
    "## CNN attempt 2 LeNet5 reference\n",
    "## conv default stride = 1, padding  = 0\n",
    "class ConvNet(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(ConvNet, self).__init__()\n",
    "        self.CLayers = nn.Sequential(\n",
    "        # Calculations *Andrew Ng Youtube:\n",
    "        # https://www.youtube.com/watch?v=3PyJA9AfwSk\n",
    "        # [(n + 2p -f)/s] +1 \n",
    "        \n",
    "        # Convolution block one with 2 layers\n",
    "        nn.Conv2d(in_channels=3, out_channels=32,kernel_size=3),\n",
    "        nn.BatchNorm2d(32),\n",
    "        nn.ELU(),\n",
    "        nn.Conv2d(in_channels=32, out_channels=32,kernel_size=3),\n",
    "        nn.ELU(),\n",
    "        nn.MaxPool2d(kernel_size=2, stride=2, ceil_mode=True),\n",
    "        nn.Dropout(0.5),\n",
    "\n",
    "        # Convolution block two with 2 layers\n",
    "        nn.Conv2d(in_channels=32, out_channels=64,kernel_size=3),\n",
    "        nn.BatchNorm2d(64),\n",
    "        nn.ELU(),\n",
    "        nn.Conv2d(in_channels=64, out_channels=64,kernel_size=3),\n",
    "        nn.ELU(),\n",
    "        nn.MaxPool2d(kernel_size=2, stride=2, ceil_mode=True),\n",
    "        nn.Dropout(0.5),\n",
    "        )\n",
    "        \n",
    "        self.LinLayers = nn.Sequential(\n",
    "        nn.Linear(3072, 512),\n",
    "        nn.ELU(),\n",
    "        # add dropout before final layer to deal with overfitting\n",
    "        # promote independence between feature maps\n",
    "        nn.Dropout(0.25),\n",
    "        nn.Linear(512, num_classes)\n",
    "        )\n",
    "        \n",
    "    # Output tensor\n",
    "    def forward(self, x):\n",
    "        self.CLayers(x)\n",
    "        x = x.view(x.size(0), -1)\n",
    "        return self.LinLayers(x)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ca4dec2d",
   "metadata": {},
   "source": [
    "Below are the functions to run either a hyperparamter grid search for ConvNet or a Learning rate search. No model initialization needed beforehand."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "21500235",
   "metadata": {},
   "outputs": [],
   "source": [
    "# %%time\n",
    "# Cvalue, CDict = GridSearch('CN')\n",
    "# print(Cvalue)\n",
    "# print(CDict[Cvalue])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "955403c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # LR selection for CNN\n",
    "# def CNRateSelection(param):\n",
    "#     lr_list = []\n",
    "#     for p in param:\n",
    "#         #for each learning rate, instantiate a new model\n",
    "#         model = ConvNet()\n",
    "#         model = model.to(device)\n",
    "#         train_loss = LRFind(model, p)\n",
    "#         lr_list.append(train_loss)\n",
    "    \n",
    "#     return lr_list\n",
    "\n",
    "# # Testing LR hyperparameters\n",
    "# parameters = [1e-3, 5e-3, 1e-4, 5e-4, 1e-5, 5e-5, 1e-6]\n",
    "\n",
    "# CNArray = CNRateSelection(parameters)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "496a7f90",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "model = ConvNet()\n",
    "model = model.to(device)\n",
    "print(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f74274e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "## ~45mins for 100 epochs\n",
    "# w,x,y,z = train(model, 1e-4)\n",
    "w,x,y,z = train_lrs(model)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "10723b50",
   "metadata": {},
   "source": [
    "Barebone accuracy of 26% on 20 epochs. Accuracy of 19.6 test and 22.4 training with data augmentation and learning rate scheduler."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fdee5bba",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Evaluating model with test set ##\n",
    "test_acc = evaluate(testloader, model)\n",
    "train_acc = evaluate(trainloader, model)\n",
    "print(test_acc, train_acc)\n",
    "\n",
    "_ = top_evaluate(testloader, model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6c284791",
   "metadata": {},
   "outputs": [],
   "source": [
    "# del model\n",
    "# experiment.end()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "21f5f775",
   "metadata": {},
   "outputs": [],
   "source": [
    "# experiment = Experiment(\n",
    "#     api_key=\"dBen8W4vvf6ErSgyIaZZhL9UG\",\n",
    "#     project_name=\"dl\",\n",
    "#     workspace=\"nguyensome\",\n",
    "# )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3f1fceb4",
   "metadata": {},
   "source": [
    "Part 3: GoogLeNet - torchvision model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "998c9a1a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torchvision.models as models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "18762c7d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# %%time\n",
    "# Gvalue, GDict = GridSearch('GN')\n",
    "# print(Gvalue)\n",
    "# print(GDict[Gvalue])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d109e379",
   "metadata": {},
   "outputs": [],
   "source": [
    "modelGLN = models.googlenet(aux_logits = False)\n",
    "for param in modelGLN.parameters():\n",
    "    param.grad_requires = False\n",
    "    \n",
    "in_features = modelGLN.fc.in_features\n",
    "modelGLN.fc = nn.Linear(in_features, 100, bias=True)\n",
    "modelGLN = modelGLN.to(device)\n",
    "print(modelGLN)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f9de3a38",
   "metadata": {},
   "source": [
    "Best rate found to be 1e-4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2402eb61",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # LR selection for GLN\n",
    "# def GLNRateSelection(param):\n",
    "#     lr_list = []\n",
    "#     for p in param:\n",
    "#         #for each learning rate, instantiate a new model\n",
    "#         modelGLN = models.googlenet(aux_logits = False)\n",
    "#         for param in modelGLN.parameters():\n",
    "#             param.grad_requires = False\n",
    "\n",
    "#         in_features = modelGLN.fc.in_features\n",
    "#         modelGLN.fc = nn.Linear(in_features, 100, bias=True)\n",
    "#         modelGLN = modelGLN.to(device)\n",
    "#         train_loss = LRFind(modelGLN, p)\n",
    "#         lr_list.append(train_loss)\n",
    "    \n",
    "#     return lr_list\n",
    "# params = [1e-2, 1e-3, 1e-4, 5e-4, 1e-5, 5e-5, 1e-6]\n",
    "# # Testing LR hyperparameters\n",
    "# GLNArray = GLNRateSelection(params)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d6d46f4a",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "%%time\n",
    "## One hour~ for 100 Epochs\n",
    "# w,x,y,z = train(modelGLN, 1e-4)\n",
    "w,x,y,z = train_lrs(modelGLN)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e89873a0",
   "metadata": {},
   "source": [
    "Barebone accuracy of 35% on 20 epochs. Only randomerasing had accuracy of test 0.371 and train 0.480. Still some overfitting issues here. Added a second data augmentation method, results was 0.37 and 0.42 test and train.\n",
    "\n",
    "Model significantly overfits after 50 epochs with 47.5% testing and 76.5% training accuracy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fc51ba9e",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Evaluating model with test set ##\n",
    "test_acc = evaluate(testloader, modelGLN)\n",
    "train_acc = evaluate(trainloader, modelGLN)\n",
    "print(test_acc, train_acc)\n",
    "\n",
    "_ = top_evaluate(testloader, modelGLN)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3765c8e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# del modelGLN"
   ]
  },
  {
   "cell_type": "raw",
   "id": "c2488a12",
   "metadata": {},
   "source": [
    "Extra exploration: Fast Gradient Sign method adversarial attack. \n",
    "\n",
    "Reference : https://pytorch.org/tutorials/beginner/fgsm_tutorial.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dce68d9a",
   "metadata": {},
   "outputs": [],
   "source": [
    "epsilons = [0, .05, .1, .15, .2, .25, .3] \n",
    "classes = ('beaver', 'dolphin', 'otter', 'seal', 'whale',\n",
    "           'aquarium fish', 'flatfish', 'ray', 'shark', 'trout',\n",
    "           'orchids', 'poppies', 'roses', 'sunflowers', 'tulips',\n",
    "           'bottles', 'bowls', 'cans', 'cups', 'plates',\n",
    "           'apples', 'mushrooms', 'oranges', 'pears', 'sweet peppers',\n",
    "           'clock', 'computer keyboard', 'lamp', 'telephone', 'television',\n",
    "           'bed', 'chair', 'couch', 'table', 'wardrobe',\n",
    "           'bee', 'beetle', 'butterfly', 'caterpillar', 'cockroach',\n",
    "           'bear', 'leopard', 'lion', 'tiger', 'wolf',\n",
    "           'bridge', 'castle', 'house', 'road', 'skyscraper', \n",
    "           'cloud', 'forest', 'mountain', 'plain', 'sea',\n",
    "           'camel', 'cattle', 'chimpanzee', 'elephant', 'kangaroo',\n",
    "           'fox', 'porcupine', 'possum', 'raccoon', 'skunk',\n",
    "           'crab', 'lobster', 'snail', 'spider', 'worm',\n",
    "           'baby', 'boy', 'girl', 'man', 'woman',\n",
    "           'crocodile', 'dinosaur', 'lizard', 'snake', 'turtle',\n",
    "           'hamster', 'mouse', 'rabbit', 'shrew', 'squirrel',\n",
    "           'maple', 'oak', 'palm', 'pine', 'willow',\n",
    "           'bicycle', 'bus', 'motorcycle', 'pickup' 'truck', 'train',\n",
    "           'lawn-mower', 'rocket', 'streetcar', 'tank', 'tractor'\n",
    "          )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c5707697",
   "metadata": {},
   "outputs": [],
   "source": [
    "def fgsm_attack(model, image, target, epsilon):\n",
    "    image.requires_grad = True\n",
    "\n",
    "    output = model(image)\n",
    "    pred = output.max(1, keepdim=True)[1] \n",
    "    \n",
    "    # Ignore incorrect predictions\n",
    "    if pred[0] != target[0]:\n",
    "        return image\n",
    "    \n",
    "    # Negative likelihood loss used by FGSM\n",
    "    loss = F.nll_loss(F.log_softmax(output), target)\n",
    "    model.zero_grad()\n",
    "    loss.backward()\n",
    "\n",
    "    data = image.grad.data\n",
    "    # sign of gradient needed to know which directions to move\n",
    "    sign_data_grad = data.sign()\n",
    "    \n",
    "    perturbed_image = image + epsilon*sign_data_grad\n",
    "    perturbed_image = torch.clamp(perturbed_image, 0, 1)\n",
    "    \n",
    "    return perturbed_image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "14fa88b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "def attack(model, epsilon):\n",
    "\n",
    "    correct = 0\n",
    "    adversary = []  \n",
    "    original = []  \n",
    "\n",
    "    for image, target in testloader:\n",
    "\n",
    "        image = image.to(device)\n",
    "        target = target.to(device)\n",
    "        # Forward pass the data through the model\n",
    "        output = model(image)\n",
    "        pred = output.max(1, keepdim=True)[1] \n",
    "        \n",
    "        perturbed_data = fgsm_attack(model, image, target, epsilon=epsilon)\n",
    "        \n",
    "        # Classify perturbed image\n",
    "        output = model(perturbed_data)\n",
    "\n",
    "        final_pred = output.max(1, keepdim=True)[1] \n",
    "        # No effect\n",
    "        if final_pred[0].item() == target[0].item(): \n",
    "            correct += 1\n",
    "            \n",
    "            # 0 epsilon examples \n",
    "            if (epsilon == 0) and (len(adversary) < 5):\n",
    "                adv_ex = perturbed_data.squeeze().detach().cpu().numpy()\n",
    "                adversary.append((pred[0].item(), final_pred[0].item(), adv_ex))\n",
    "                ori_ex = image.squeeze().detach().cpu().numpy()\n",
    "                original.append((target[0].item(), pred[0].item(), ori_ex))\n",
    "        else:\n",
    "            if len(adversary) < 5:\n",
    "                adv_ex = perturbed_data.squeeze().detach().cpu().numpy()\n",
    "                adversary.append((pred[0].item(), final_pred[0].item(), adv_ex))\n",
    "                ori_ex = image.squeeze().detach().cpu().numpy()\n",
    "                original.append((target[0].item(), pred[0].item(), ori_ex))\n",
    "\n",
    "    # Calculate final accuracy for this epsilon\n",
    "    final_acc = correct/(len(testloader))\n",
    "    print(\"Epsilon: {} | Test Accuracy = {} | {} = {}\".format(epsilon, correct, len(testloader), final_acc))\n",
    "\n",
    "    return final_acc, adversary, original"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "edff614c",
   "metadata": {},
   "outputs": [],
   "source": [
    "#FGSM attack\n",
    "accuracies = [] \n",
    "adversary = [] \n",
    "original = []\n",
    "\n",
    "# Run test for each epsilon\n",
    "for eps in epsilons:\n",
    "    acc, adv, orig = attack(modelGLN, eps)\n",
    "    accuracies.append(acc)\n",
    "    adversary.append(adv)\n",
    "    original.append(orig)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bed28fb0",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Accuracy after attack vs epsilon\n",
    "plt.figure(figsize=(5,5))\n",
    "plt.plot(epsilons, accuracies, \"*-\")\n",
    "plt.yticks(np.arange(0, 1.1, step=0.1))\n",
    "plt.xticks(np.arange(0, .35, step=0.05))\n",
    "plt.title(\"CIFAR Model Accuracy vs Epsilon\")\n",
    "plt.xlabel(\"Epsilon\")\n",
    "plt.ylabel(\"Accuracy\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "953195c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot several examples vs their adversarial samples at each epsilon for fgms attack\n",
    "cnt = 0\n",
    "plt.figure(figsize=(8,20))\n",
    "for i in range(len(epsilons)):\n",
    "    for j in range(len(adversary[i])):\n",
    "        cnt += 1\n",
    "        plt.subplot(len(epsilons),len(adversary[0]),cnt)\n",
    "        plt.xticks([], [])\n",
    "        plt.yticks([], [])\n",
    "        if j==0:\n",
    "                plt.ylabel(\"Eps: {}\".format(epsilons[i]), fontsize=14)\n",
    "        orig,adv,ex = adversary[i][0]\n",
    "        plt.title(\"{} -> {}\".format(classes[orig], classes[adv]))\n",
    "        plt.imshow(ex[0].transpose(1,2,0), cmap=\"gray\")\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
